{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FuzzySearch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cw00dw0rd/ArangoDBUniversity/blob/master/FuzzySearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bfxiSjWR8Kmw"
      },
      "source": [
        "![arangodb](https://github.com/joerg84/ArangoDBUniversity/blob/master/img/ArangoDB_logo.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VE3KR8sW8Kmw"
      },
      "source": [
        "# Fuzzy Search "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c3DiEFJE8Kmx"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joerg84/ArangoDBUniversity/blob/master/FuzzySearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4o7Vn4Uo8Kmy"
      },
      "source": [
        "[ArangoSearch](https://www.arangodb.com/why-arangodb/full-text-search-engine-arangosearch/) provides information retrieval features, natively integrated into ArangoDB’s query language and with support for all data models. It is primarily a full-text search engine, a much more powerful alternative to the full-text index type.\n",
        "Check this [ArangoSearch notebook](https://colab.research.google.com/github/joerg84/ArangoDBUniversity/blob/master/ArangoSearch.ipynb) for an introduction to ArangoSearch.\n",
        "\n",
        "When dealing with real-world text retrieval, we often not only care about exact matches to our search phrase but need to consider for example typos or alternative spellings.\n",
        "“Fuzzy search” is an umbrella term referring to a set of algorithms for such approximate matching. Usually such algorithms evaluate some similarity measure showing how close a search term is to the items in a dictionary. Then a search engine can make a decision on which results have to be shown first.\n",
        "\n",
        "In this notebook we will explore the different implementations of fuzzy search in [ArangoSearch](https://www.arangodb.com/why-arangodb/full-text-search-engine-arangosearch/):\n",
        "* [NGram Similarity](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_match)\n",
        "* [NGram Positional Similarity](https://www.arangodb.com/docs/devel/aql/functions-string.html#ngram_positional_similarity)\n",
        "* [NGram Match](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_match)\n",
        "* [Levenshtein Distance](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match\n",
        ")\n",
        "* [Levenshtein Match](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZlcbVfOs8Kmy"
      },
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uoIFBPwp8Kmy"
      },
      "source": [
        "Before getting started with ArangoSearch we need to prepare our environment and create a temporary database on ArangoDB's managed Service Oasis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uXdL1FZe8Kmz",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/joerg84/ArangoDBUniversity.git\n",
        "!rsync -av ArangoDBUniversity/ ./ --exclude=.git\n",
        "!pip3 install pyarango\n",
        "!pip3 install \"python-arango>=5.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1pKXjdTS8Km2",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import sys\n",
        "import oasis\n",
        "import time\n",
        "\n",
        "from pyArango.connection import *\n",
        "from arango import ArangoClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6968hvSs8Km3"
      },
      "source": [
        "Create the temporary database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TaGHLin28Km4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1100c6aa-7481-4603-ef22-24ca22ceca6f"
      },
      "source": [
        "# Retrieve tmp credentials from ArangoDB Tutorial Service\n",
        "login = oasis.getTempCredentials(tutorialName=\"FuzzyArangoSearch\", credentialProvider=\"https://de64d9dc6b66.arangodb.cloud:8529/_db/_system/tutorialDB/tutorialDB\")\n",
        "\n",
        "# Connect to the temp database\n",
        "# Please note that we use the python-arango driver as it has better support for ArangoSearch \n",
        "database = oasis.connect_python_arango(login)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requesting new temp credentials.\n",
            "Temp database ready to use.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ECAfpWU48Km6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fc39c8a5-bd7f-4d6b-9350-18f2c692120b"
      },
      "source": [
        "print(\"https://\"+login[\"hostname\"]+\":\"+str(login[\"port\"]))\n",
        "print(\"Username: \" + login[\"username\"])\n",
        "print(\"Password: \" + login[\"password\"])\n",
        "print(\"Database: \" + login[\"dbName\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://de64d9dc6b66.arangodb.cloud:8529\n",
            "Username: TUTjg0g27gptairsbxejexmnr\n",
            "Password: TUT64w8rn9q5btw0egpiwetrr\n",
            "Database: TUT3vjrd43f8oaqcxomaui4x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xZ0mydlq8Km8"
      },
      "source": [
        "Feel free to use to above URL to checkout the ArangoDB WebUI!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_vXqUK6L8Km9"
      },
      "source": [
        "##  IMDB Example Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o9JQShDi8Km9"
      },
      "source": [
        "![imdb](https://github.com/joerg84/ArangoDBUniversity/blob/master/img/IMDB_graph.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SXkaey-g8Km9"
      },
      "source": [
        "Last, but not least we will import the [IMBD Example Dataset](https://github.com/arangodb/example-datasets/tree/master/Graphs/IMDB) including information about various movies, actors, directors, ... as a graph. \n",
        "*Note the included arangorestore will only work on Linux or Windows systems, if you want to run this notebook on a different OS please consider using the appropriate arangorestore from the [Download area](https://www.arangodb.com/download-major/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bKM6jcXa8Km-"
      },
      "source": [
        "## Linux:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VPqigG8H8Km-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "5d0e8a80-3c5d-4057-c208-37e4c5369e06"
      },
      "source": [
        "! ./tools/arangorestore -c none --server.endpoint http+ssl://{login[\"hostname\"]}:{login[\"port\"]} --server.username {login[\"username\"]} --server.database {login[\"dbName\"]} --server.password {login[\"password\"]} --default-replication-factor 3  --input-directory \"data/imdb\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m2020-07-15T00:45:46Z [282] INFO [05c30] {restore} Connected to ArangoDB 'http+ssl://de64d9dc6b66.arangodb.cloud:8529'\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [3b6a4] {restore} no properties object\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [9b414] {restore} # Re-creating document collection 'imdb_vertices'...\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [9b414] {restore} # Re-creating edge collection 'imdb_edges'...\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [6d69f] {restore} # Dispatched 2 job(s), using 2 worker(s)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [94913] {restore} # Loading data into edge collection 'imdb_edges', data size: 48957903 byte(s)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [d88c6] {restore} # Creating indexes for collection 'imdb_vertices'...\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:47Z [282] INFO [94913] {restore} # Loading data into document collection 'imdb_vertices', data size: 22665786 byte(s)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:52Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 16777216 byte(s) from datafiles, sent 2 data batch(es) of 0 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:45:57Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 16777216 byte(s) from datafiles, sent 2 data batch(es) of 0 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:02Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 25165824 byte(s) from datafiles, sent 3 data batch(es) of 8388598 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:07Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 33554432 byte(s) from datafiles, sent 4 data batch(es) of 16777139 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:11Z [282] INFO [69a73] {restore} # Still loading data into edge collection 'imdb_edges', 16777216 of 48957903 byte(s) restored (34 %)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:12Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 41943040 byte(s) from datafiles, sent 5 data batch(es) of 25165735 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:17Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 41943040 byte(s) from datafiles, sent 5 data batch(es) of 25165735 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:22Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 41943040 byte(s) from datafiles, sent 5 data batch(es) of 25165735 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:26Z [282] INFO [69a73] {restore} # Still loading data into document collection 'imdb_vertices', 16777216 of 22665786 byte(s) restored (74 %)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:27Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 47831610 byte(s) from datafiles, sent 6 data batch(es) of 33554336 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:32Z [282] INFO [75e65] {restore} # Current restore progress: restored 0 of 2 collection(s), read 56220218 byte(s) from datafiles, sent 7 data batch(es) of 41942766 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:37Z [282] INFO [6ae09] {restore} # Successfully restored document collection 'imdb_vertices'\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:37Z [282] INFO [75e65] {restore} # Current restore progress: restored 1 of 2 collection(s), read 56220218 byte(s) from datafiles, sent 7 data batch(es) of 47831409 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:39Z [282] INFO [69a73] {restore} # Still loading data into edge collection 'imdb_edges', 33554432 of 48957903 byte(s) restored (68 %)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:42Z [282] INFO [75e65] {restore} # Current restore progress: restored 1 of 2 collection(s), read 64608826 byte(s) from datafiles, sent 8 data batch(es) of 56220084 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:47Z [282] INFO [75e65] {restore} # Current restore progress: restored 1 of 2 collection(s), read 64608826 byte(s) from datafiles, sent 8 data batch(es) of 56220084 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:52Z [282] INFO [75e65] {restore} # Current restore progress: restored 1 of 2 collection(s), read 71623689 byte(s) from datafiles, sent 9 data batch(es) of 64608683 byte(s) total size, queued jobs: 0, workers: 2\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:54Z [282] INFO [69a73] {restore} # Still loading data into edge collection 'imdb_edges', 48957903 of 48957903 byte(s) restored (100 %)\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:54Z [282] INFO [6ae09] {restore} # Successfully restored edge collection 'imdb_edges'\n",
            "\u001b[0m\u001b[0m2020-07-15T00:46:54Z [282] INFO [a66e1] {restore} Processed 2 collection(s) in 68.576594 s, read 71623689 byte(s) from datafiles, sent 9 data batch(es) of 71623687 byte(s) total size\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9yE_uMym8KnA"
      },
      "source": [
        "# Create First View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GhQynyJJ8KnB"
      },
      "source": [
        "As discussed above, an ArangoSearch view contains references to documents stored in different collections. \n",
        "This makes it possible to perform complex federated searches, even over a complete graph including vertex and edge collections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-koXo6C8KnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "1a9c3fcd-4a5b-49c3-e6f9-3754a99b21f2"
      },
      "source": [
        "# Create an ArangoSearch view.\n",
        "database.create_arangosearch_view(\n",
        "    name='v_imdb'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ViewCreateError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mViewCreateError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e4be2c9ca12b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create an ArangoSearch view.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m database.create_arangosearch_view(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v_imdb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arango/database.py\u001b[0m in \u001b[0;36mcreate_arangosearch_view\u001b[0;34m(self, name, properties)\u001b[0m\n\u001b[1;32m   2198\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mViewCreateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_arangosearch_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arango/api.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, request, response_handler)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0municode\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arango/executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, request, response_handler)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[1;32m     81\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arango/database.py\u001b[0m in \u001b[0;36mresponse_handler\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mViewCreateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mViewCreateError\u001b[0m: [HTTP 409][ERR 1207] duplicate view name 'v_imdb'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dn3rKYKG8KnD"
      },
      "source": [
        "Let us check it is actually there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5bwOthX8KnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e0dd62d-d6c0-43fe-a10a-dc9e296727d2"
      },
      "source": [
        "print(database[\"v_imdb\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<StandardCollection v_imdb>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7RJtPcu8KnF"
      },
      "source": [
        "Next, we will create a [custom analyzer](https://www.arangodb.com/docs/stable/arangosearch-analyzers.html) to preprocess the values.\n",
        "Note that, in order to support ngram similarity the analyzer must have:\n",
        "* At least the \"position\" and \"frequency\" features enabled\n",
        "* The same min and max values\n",
        "* preserveOriginal set to False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SSfISdj8kpd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e5cd9da7-8742-4853-d469-700195b8ae29"
      },
      "source": [
        "# Delete in case analyzer existed before\n",
        "database.delete_analyzer('fuzzy_search_bigram', ignore_missing=True)\n",
        "\n",
        "database.create_analyzer(\n",
        "        name='fuzzy_search_bigram',\n",
        "        analyzer_type='ngram',\n",
        "        properties={  \n",
        "        \"min\": 2,  \n",
        "        \"max\": 2,  \n",
        "        \"preserveOriginal\": False \n",
        "        }, \n",
        "        features=[\"position\", \"frequency\"] \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'features': ['position', 'frequency'],\n",
              " 'name': 'TUT3vjrd43f8oaqcxomaui4x::fuzzy_search_bigram',\n",
              " 'properties': {'endMarker': '',\n",
              "  'max': 2,\n",
              "  'min': 2,\n",
              "  'preserveOriginal': False,\n",
              "  'startMarker': '',\n",
              "  'streamType': 'binary'},\n",
              " 'type': 'ngram'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EiCHaBtl4gkZ"
      },
      "source": [
        "Next, we need to link the view and our custom analyzer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IL8GQTtQ8KnF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "80a6c4ef-0e3e-4155-9fc8-2b2c64d3709a"
      },
      "source": [
        " link = { \n",
        "  \"includeAllFields\": True,\n",
        "  \"fields\" : { \n",
        "      \"title\" : { \"analyzers\" : [ \"fuzzy_search_bigram\"] },\n",
        "      \"description\" : { \"analyzers\" : [ \"fuzzy_search_bigram\"] }\n",
        "      }\n",
        "}\n",
        "\n",
        "\n",
        "database.update_arangosearch_view(\n",
        "    name='v_imdb',\n",
        "    properties={'links': { 'imdb_vertices': link }}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cleanup_interval_step': 2,\n",
              " 'commit_interval_msec': 1000,\n",
              " 'consolidation_interval_msec': 10000,\n",
              " 'consolidation_policy': {'min_score': 0,\n",
              "  'segments_bytes_floor': 2097152,\n",
              "  'segments_bytes_max': 5368709120,\n",
              "  'segments_max': 10,\n",
              "  'segments_min': 1,\n",
              "  'type': 'tier'},\n",
              " 'global_id': 'c30010319/',\n",
              " 'id': '30010319',\n",
              " 'links': [{}],\n",
              " 'name': 'v_imdb',\n",
              " 'primary_sort': [],\n",
              " 'type': 'arangosearch',\n",
              " 'writebuffer_active': 0,\n",
              " 'writebuffer_idle': 64,\n",
              " 'writebuffer_max_size': 33554432}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMZumewz8KnH"
      },
      "source": [
        "As the indexing might take a few seconds, let us have a brief look at what is actually going on.\n",
        "\n",
        "When you link a collection you can choose which individual fields to link or specify to link all fields. It might be helpful to think about linking fields in the same way you think about indexing attributes, although not exactly the same. When you link data to a view it is indexed in a way that allows for quick retrieval. This process also stores the data in a way that allows for the ArangoSearch-specific AQL functions to perform unique queries such as tokenizing, stemming, removing stop words, and as we will see in this notebook complex matching functions.\n",
        "\n",
        "An additional benefit and a difference to typical indexing is that you are able to link multiple collections to one view and apply the desired analyzers. The image below shows how the collections are linked, analyzed and then made available via the view. When performing queries you can use all the typical AQL functions against a view, the same way that you would with a collection name. Though, the real benefit comes when using ArangoSearch-specific functions and you start taking advanatage of features such as ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "89Xus7i28KnI"
      },
      "source": [
        "![ArangoSearch](https://github.com/joerg84/ArangoDBUniversity/blob/master/img/ArangoSearch_Arch.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w8LsVW-LNIIU"
      },
      "source": [
        "By now our view should be ready, so let us issue the first test query and look for short Drama Movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "53oLsRFJNIZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "69db6e69-c5fc-4498-c5ab-f485f13eb64e"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "  \"\"\"\n",
        "  FOR d IN v_imdb \n",
        "    SEARCH d.type == \"Movie\" \n",
        "    AND \n",
        "    d.genre == \"Drama\" \n",
        "    AND \n",
        "    d.runtime IN 10..50 \n",
        "    RETURN d.title\n",
        "  \"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "print('\\033[4mMovie Titles\\033[0m ')\n",
        "\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[4mMovie Titles\u001b[0m \n",
            "Frühlings Erwachen - Eine Kindertragödie\n",
            "Glastage\n",
            "Sunday in August\n",
            "Land gewinnen\n",
            "À San Remo\n",
            "8½\n",
            "Zwischen Flieder wandern und singen\n",
            "Silvester Home Run\n",
            "Bis zur Unendlichkeit\n",
            "Space Riders\n",
            "Dreamcatcher\n",
            "Rounds\n",
            "Hotel Chevalier\n",
            "The Wiggles: Wiggle Bay\n",
            "VeggieTales: An Easter Carol\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YFhVZYhcSejU"
      },
      "source": [
        "If we set up everything correctly there should be 18 results, containing movies such as:\n",
        "  * Frühlings Erwachen - Eine Kindertragödie\n",
        "  * Glastage\n",
        "  * Sunday in August"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kInsH7LYSiEF"
      },
      "source": [
        "Now that we have finished some of the setup, let's move on to the functions that make up Fuzzy search. \n",
        "As mentioned in the beginning of this notebook, Fuzzy search comes in the form of various [NGram similarity](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_match) and [Levenshtein distance](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match\n",
        ") AQL functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SF3oT3gQfbB0"
      },
      "source": [
        "# NGram Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "haexui4sDce2"
      },
      "source": [
        "`NGRAM_SIMILARITY(input, target, ngramSize) → similarity`\n",
        "\n",
        "NGram similarity is a measure for the difference between two strings represented by counting how long the longest sequence of matching ngrams is, divided by target’s total ngram count. To better understand this concept let's start with a simple example. The below query compares the phrase `quick fox` to the similar phrase of `quick foxx` (additional `x`). These are similar phrases and as such, they should have a high ngram similarity. \n",
        "\n",
        "```\n",
        "Case Conversion Note:\n",
        "NGram analyzers do not currently support case conversion. \n",
        "This means that the data stored needs to match the search terms case.\n",
        "```\n",
        "\n",
        "Go ahead and execute the query below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t4-VhEhgC6oM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47a4c548-eb60-4834-b9f0-4bc015ff1ee0"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN NGRAM_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quick foxx\", \n",
        "2)\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mNGRAM_SIMILARITY\\033[0m ')\n",
        "  print(doc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[4mNGRAM_SIMILARITY\u001b[0m \n",
            "0.8888888955116272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HQlOtFJCfglL"
      },
      "source": [
        "With a ngram size of 2, the ngram similarity between both strings is 0.888, the closer the similarity is to 1 the more similar they are. Feel free experiment with other combinations such as `NGRAM_SIMILARITY( \"same string\",\"same string\", 2)` or vary the ngramSize.\n",
        "\n",
        "NGram functions such as this break apart the words using the supplied ngram size, 2 in our query above. This means that the function compares the two words broken up into their 2 letter ngrams:\n",
        "\n",
        "  ```\n",
        "  quick fox         --         quick foxx\n",
        "  ----------------------------------------\n",
        "  qu                --         qu (match)\n",
        "  ui                --         ui (match)\n",
        "  ic                --         ic (match)\n",
        "  ck                --         ck (match)\n",
        "  k                 --         k  (match)\n",
        "   f                --          f (match)\n",
        "  fo                --         fo (match)\n",
        "  ox                --         ox (match)\n",
        "  x                 --         xx (do not match)\n",
        "  ```\n",
        "If we use simple math here we can see that there is around an 85% match when an extra `x` is supplied. However, N-Gram Similarity and Distance is not as simple as this, but hopefully this provides a quick intro to the basic concept of ngram matching and similarity. If you would like to take a deep dive into this topic, a paper published by [Grzegorz Kondrak at the University of Alberta](https://webdocs.cs.ualberta.ca/~kondrak/papers/spire05.pdf) is a great resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "POtg36jBTh5r"
      },
      "source": [
        "### NGram Positional Similarity\n",
        "`NGRAM_POSITIONAL_SIMILARITY(input, target, ngramSize) → similarity`\n",
        "\n",
        "While [NGRAM_SIMILARITY()](https://www.arangodb.com/docs/devel/aql/functions-string.html#ngram_similarity) only counts fully matching ngrams, [NGRAM_POSITIONAL_SIMILARITY()](https://www.arangodb.com/docs/devel/aql/functions-string.html#ngram_positional_similarity) also considers partially matching ones. Let us look at how that effects the returned scores. \n",
        "In this first example we are comparing `NGRAM_SIMILARITY` and `NGRAM_POSITIONAL_SIMILARITY` scores using the same two phrases as with our previous  example. These phrases are so similar that counting partial matches doesn't make any difference, thus we get the same scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1Uj0IBbTlmR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b510733d-41c9-4608-f6ef-876c33412d51"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN\n",
        "{\"NGRAM_SIMILARITY\" : NGRAM_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quick foxx\", \n",
        "3),\n",
        "\"NGRAM_POSITIONAL_SIMILARITY\" : NGRAM_POSITIONAL_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quick foxx\", \n",
        "3)}\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mNGRAM_SIMILARITY\\033[0m ', '\\033[4mNGRAM_POSITIONAL_SIMILARITY\\033[0m '.rjust(44))\n",
        "  print(doc['NGRAM_SIMILARITY'], str(doc['NGRAM_POSITIONAL_SIMILARITY']).rjust(25))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[4mNGRAM_SIMILARITY\u001b[0m          \u001b[4mNGRAM_POSITIONAL_SIMILARITY\u001b[0m \n",
            "0.875                     0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "izAPzIB8Tpw4"
      },
      "source": [
        "If we start to change a few more letters in the phrases, the differences between the two functions becomes more clear. The score for `NGRAM_POSITIONAL_SIMILARITY` is nearly double that of `NGRAM_SIMILARITY`, due to the fact that it counted the partial matches. This provides us with some additional 'fuzziness' by allowing the matching requirement to be a bit more lenient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b1CgLWTeTtSF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e32232fb-0661-4a8f-d1dc-32e034c40baf"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN\n",
        "{\"NGRAM_SIMILARITY\" : NGRAM_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quirky foxx\", \n",
        "3),\n",
        "\"NGRAM_POSITIONAL_SIMILARITY\" : NGRAM_POSITIONAL_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quirky foxx\", \n",
        "3)}\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mNGRAM_SIMILARITY\\033[0m ', '\\033[4mNGRAM_POSITIONAL_SIMILARITY\\033[0m '.rjust(44))\n",
        "  print(doc['NGRAM_SIMILARITY'], str(doc['NGRAM_POSITIONAL_SIMILARITY']).rjust(25))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[4mNGRAM_SIMILARITY\u001b[0m          \u001b[4mNGRAM_POSITIONAL_SIMILARITY\u001b[0m \n",
            "0.3333333432674408        0.5925925970077515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z9KCiOw0TwSE"
      },
      "source": [
        "Depending on your requirements, the decision to count partially matching ngrams adds some 'fuzziness' that may help provide some context to your searches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4bZtMbZyT4Qn"
      },
      "source": [
        "[NGRAM_SIMILARITY](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_similarity) and [NGRAM_POSITIONAL_SIMILARITY](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_positional_similarity) are two new functions that come with ArangoDB 3.7 and can be used to improve text searches but have a drawback of not being able to utilize the indexing benefits of views. They are still very powerful string functions and can offer a lot of functionality for text queries.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## NGram Match\n",
        "\n",
        "`NGRAM_MATCH(path, target, threshold, analyzer)`\n",
        "\n",
        "However, [NGRAM_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_match) is able to use the indexing of ArangoSearch views and is what we will look at next.\n",
        "\n",
        "Let us start by using the [NGRAM_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#ngram_match) function to find a mispelled movie title. This should return multiple Star Wars movies, let's take a moment to understand why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b9_f99xRfvcZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f8fd8974-ec24-4f53-b2d7-56d8483e8ed3"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "FOR d IN v_imdb SEARCH NGRAM_MATCH(d.title, 'Galxy', 0.7, 'fuzzy_search_bigram')\n",
        "SORT BM25(d) DESC  \n",
        "RETURN d.title\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Galaxy Quest\n",
            "The Hitchhiker's Guide to the Galaxy\n",
            "Galaxy Express 999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9XsTF8Mtfwaj"
      },
      "source": [
        "The `NGRAM_MATCH` syntax follows typical ArangoSearch function structure. You first supply the field you would like to search, the search term(s), and then the next value is the threshold amount which must be between `0.0` and `1.0`, last is the analyzer to use on the search terms. The `.7` threshold amount is the new addition and is how much ‘fuzziness’ we are still considering to be a match.\n",
        "\n",
        "The similarity is calculated by counting how long the longest sequence of matching ngrams is, divided by the target’s total ngram count. Only fully matching ngrams are counted.\n",
        "\n",
        "The analyzer we used was configured with a min and max of 2, which means it looks at words 2 letters at a time. This is useful for determining the longest common sequence and context. The idea behind n-gram matching is searching for similar words, but not necessarily exact matches. One of the simplest ways of calculating similarity between two words is calculating the longest common sequence (LCS) of letters. The longer the LCS is the more similar the words are. However, this approach has one big disadvantage – absence of context. For example, words `connection` and `fonetica` have a long LCS (o-n-e-t-i) but very different meanings. To add some context, ngram sequences are used.\n",
        "\n",
        "Each word is split into a series of letter groups and these groups are then matched. If we use the same words, but calculate similarity based on 3-grams, an ngram with max and min of 3, we will get a better similarity measure: con-onn-nne-nec-ect-cti-tio-ion vs. fon-one-net-eti-tic-ica gives shorter LCS ( zero matches). To get rid of length differences we normalize the LCS length by word length. We calculate these matches to get a rating with a value between 0 (no match at all) and 1(fully matched). \n",
        "\n",
        "Increasing the ngram size is not always the best choice due to it also increasing the accuracy requirement of the search. Scores would be much lower for the above Star Wars search if we had chosen an ngram size of 3. We would need to decrease our threshold requirement which can have the impact of returning less relevant results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vUgcAWSpdl8l"
      },
      "source": [
        "# Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1JisqzP5dflN"
      },
      "source": [
        "ArangoDB comes with two forms of the Levenshtein matching algorithm, [LEVENSHTEIN_DISTANCE](https://www.arangodb.com/docs/devel/aql/functions-string.html#levenshtein_distance) and [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match). These AQL functions provide two similar approaches for adding 'fuzziness' to your AQL queries. While the AQL functions are similar there are some important differences, which will discuss in this section, as well as showcase some examples. \n",
        "\n",
        "### Levenshtein Distance\n",
        "`LEVENSHTEIN_DISTANCE(value1, value2) → levenshteinDistance`\n",
        "\n",
        "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) is a another measure for the difference between two strings represented by the  minimum number of single-character transformations required to move from one string to the other. Let us consider a concrete example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pr-gqDa8de1I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6227fd86-ec40-4ba6-9390-1c5e72f70731"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN LEVENSHTEIN_DISTANCE(\n",
        "\"The quick brown fox jumps over the lazy dog\", \n",
        "\"The quick black dog jumps over the brown fox\")\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print(\"Edit Distance Transoformations Required: \", doc)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Edit Distance Transoformations Required:  13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0r1uwBXLBaMk"
      },
      "source": [
        "Here we need a minimum of 13 transformations to move from one string to the other. \n",
        "Feel free to find a minimum sequence for this transformation or experiment with other combinations such as `LEVENSHTEIN_DISTANCE(\"ab\", \"ba\")`. Once the distance has been calculated it can be used in other parts of your application logic and even with the same query. \n",
        "\n",
        "This functionality provides some added control over your text analysis by handling what to do with the distance measure once you have it. However, in most situations you may prefer to find the relevance, distance, and determine if the keywords or phrases match some user supplied input, with one function or statement. This functionality is where Levenshtein Match comes in and is what we will review next.\n",
        "\n",
        "Before we go and as a nice transition to looking at Levenshtein Match, here are some key differences:\n",
        "* Distance is considered a string function, not tied to ArangoSearch\n",
        "* Distance does not take advantage of ArangoSearch indexing\n",
        "* Distance uses Damerau and treats distance edits atomically\n",
        "* Match does not use Damerau by default, but can be optionally enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm48Mt5jplnb",
        "colab_type": "text"
      },
      "source": [
        "### Levenshtein Match\n",
        "\n",
        "\n",
        "`LEVENSHTEIN_MATCH(path, target, distance, transpositions, maxTerms)`\n",
        "\n",
        "Levenshtein match, matches documents with a Levenshtein distance lower than or equal to a distance between a document value and provided search value. This takes the power of the above Levenshtein distance function and combines it with filtering and relevance matching.\n",
        "\n",
        "```\n",
        "Analyzer Note:\n",
        "For our LEVENSHTEIN_MATCH examples we will use a text analyzer, instead of ngram, that has stemming disabled.\n",
        "Stemming is disabled as a convenience to avoid terms not matching due to a stemmed word, ie: galaxy is galaxi when stemmed.\n",
        "```\n",
        "The following code block:\n",
        "* Creates the analyzer named `en_tokenizer`\n",
        "* Updates our link definition object\n",
        "* Updates the `v_imdb` view definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMiFXQMToVQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "abbbb6b2-22f1-4d2f-e13e-00014e1d09fa"
      },
      "source": [
        " # Delete in case analyzer existed before\n",
        "database.delete_analyzer('en_tokenizer', ignore_missing=True)\n",
        "\n",
        "# Create a new english text analyzer to tokenize our text\n",
        "database.create_analyzer(\n",
        "        name='en_tokenizer',\n",
        "        analyzer_type='text',\n",
        "        properties={\n",
        "            'locale': 'en',\n",
        "            'stemming': False\n",
        "        }, \n",
        "        features=[\"position\", \"frequency\"] \n",
        "    )\n",
        "\n",
        "# Update the link definition object\n",
        " link = { \n",
        "  \"includeAllFields\": True,\n",
        "  \"fields\" : { \n",
        "      \"title\" : { \"analyzers\" : [ \"fuzzy_search_bigram\", \"en_tokenizer\"] },\n",
        "      \"description\" : { \"analyzers\" : [ \"fuzzy_search_bigram\", \"en_tokenizer\"] }\n",
        "      }\n",
        "}\n",
        "\n",
        "# Update the ArangoSearch view with the new link definition\n",
        "database.update_arangosearch_view(\n",
        "    name='v_imdb',\n",
        "    properties={'links': { 'imdb_vertices': link }}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cleanup_interval_step': 2,\n",
              " 'commit_interval_msec': 1000,\n",
              " 'consolidation_interval_msec': 10000,\n",
              " 'consolidation_policy': {'min_score': 0,\n",
              "  'segments_bytes_floor': 2097152,\n",
              "  'segments_bytes_max': 5368709120,\n",
              "  'segments_max': 10,\n",
              "  'segments_min': 1,\n",
              "  'type': 'tier'},\n",
              " 'global_id': 'c30010319/',\n",
              " 'id': '30010319',\n",
              " 'links': [{}],\n",
              " 'name': 'v_imdb',\n",
              " 'primary_sort': [],\n",
              " 'type': 'arangosearch',\n",
              " 'writebuffer_active': 0,\n",
              " 'writebuffer_idle': 64,\n",
              " 'writebuffer_max_size': 33554432}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaqVAPE-ptcs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "To continue exploring how Levenshtein Match can leverage edit distance scoring with term matching functionality, run the query below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IYzxDnGn8KnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "3190d64d-1cc8-4b91-f22a-d72688d1d8cf"
      },
      "source": [
        "# Execute the query\n",
        "cursor = database.aql.execute(\n",
        "  \"\"\"\n",
        "  FOR d IN v_imdb\n",
        "    SEARCH ANALYZER(LEVENSHTEIN_MATCH(\n",
        "      d.title, \n",
        "      'galxy', \n",
        "      2,\n",
        "      true,\n",
        "      3\n",
        "      ), \n",
        "    \"en_tokenizer\")\n",
        "    SORT BM25(d) DESC \n",
        "    LIMIT 10\n",
        "    RETURN {\n",
        "      \"Title\": d.title,\n",
        "      \"Score\": BM25(d)\n",
        "      }\n",
        "      \"\"\")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('Title: ', doc['Title'], )\n",
        "  print('Score: ', str(doc['Score']))\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title:  The Hitchhiker's Guide to the Galaxy\n",
            "Score:  6.20589017868042\n",
            " \n",
            "Title:  Galaxy Quest\n",
            "Score:  6.20589017868042\n",
            " \n",
            "Title:  Galaxy Express 999\n",
            "Score:  6.20589017868042\n",
            " \n",
            "Title:  When Harry Met Sally\n",
            "Score:  4.654417514801025\n",
            " \n",
            "Title:  The Adventures of André and Wally B.\n",
            "Score:  4.654417514801025\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnH-F3R6K-y4",
        "colab_type": "text"
      },
      "source": [
        "The above query is an example of a user searching for a movie title but their search term contains a typo. The user intended to type `galaxy` but accidentally left out an `a`, easy mistake. Thanks to the [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match) function we have accommodated this very common scenario. \n",
        "\n",
        "The edit distance to add in an `a` would be less than `2`, which is the distance supplied in this query, so the term `galaxy` is also taken into account, not just the misspelled word.\n",
        "\n",
        "The `3` supplied here is optional and specifies the max number of terms, such as `galaxy`, to take into account. The higher this number is, the more results you are likely to get, this makes sorting by relevance very important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWo6D1topWRS",
        "colab_type": "text"
      },
      "source": [
        "### Levenshtein Match + Phrase Search\n",
        "In practice it will be common to combine [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match) with other ArangoSearch AQL functions, [PHRASE](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#phrase) being a likely choice.\n",
        "The Phrase function honors the postion of the search term, where Levenshtein Match just looks for the word to exist and evaluates it based on relevance to the term.\n",
        "\n",
        "This combination is so common that Levenshtein Match comes with a second syntax style that works perfectly with PHRASE. The array sytnax variant shown in the query below allows for omitting the intial path argument, as it is already supplied by the PHRASE function. This combination gives us the best of both worlds, precise control with the flexibility of fuzzy search.\n",
        "\n",
        "This query looks for movie titles starting with the word `star` and uses [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match) to match a second word, with Damerau transposition set to true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcgHY9pLLDHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1345a792-c390-4b0a-b1ec-274c6000d60e"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "    \"\"\"\n",
        "FOR d IN v_imdb\n",
        "  SEARCH PHRASE(d.title, [ 'star', { LEVENSHTEIN_MATCH : ['wr', 2, true] } ], \"en_tokenizer\")\n",
        "    SORT BM25(d) DESC \n",
        "    RETURN d.title\n",
        "    \"\"\")\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars: Episode II - Attack of the Clones\n",
            "Star Wars: Episode IV - A New Hope\n",
            "Star Wars: Episode VI - Return of the Jedi\n",
            "Star Wars: Episode I - The Phantom Menace\n",
            "Star Wars Collection\n",
            "Star Wars: Episode V: The Empire Strikes Back\n",
            "Star Wars: Episode III: Revenge of the Sith\n",
            "Star Wars: The Clone Wars\n",
            "Star Wars: Revelations\n",
            "The Star Wars Holiday Special\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbxujkbYupoM",
        "colab_type": "text"
      },
      "source": [
        "You can continue using multiple [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#levenshtein_match) functions even in a single Phrase statement. This makes it possible to search for phrases where every word possibly has a typo, along with the additonal [PHRASE options](https://www.arangodb.com/docs/devel/aql/functions-arangosearch.html#phrase) such as skipTokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AXztvTMLGNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "096b2771-c969-4dd9-db0b-4f9357f5fd0f"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "FOR d IN v_imdb\n",
        "  SEARCH PHRASE(d.title, { LEVENSHTEIN_MATCH : ['lrd', 1, true] }, 2, { LEVENSHTEIN_MATCH : ['rng', 2, true] }, \"en_tokenizer\")\n",
        "    SORT BM25(d) DESC \n",
        "    RETURN d.title\n",
        "\"\"\")\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Lord of the Rings: The Two Towers\n",
            "J.R.R. Tolkien's The Lord of the Rings\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings: The Return of the King\n",
            "The Lord of the Rings Trilogy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tWpH7oVb8KnW"
      },
      "source": [
        "# Further Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dP0KpLrK8KnW"
      },
      "source": [
        "Hopefully, you can now see the potential that fuzzy search has with ArangoSearch. If you would like to continue learning more about ArangoDB and ArangoSearch here are some great next steps to get you started!\n",
        "\n",
        "* ArangoSearch Documentation\n",
        " * https://www.arangodb.com/docs/stable/arangosearch.html\n",
        "\n",
        "* ArangoSearch Training Center\n",
        " * https://www.arangodb.com/arangodb-training-center/search/arangosearch/"
      ]
    }
  ]
}